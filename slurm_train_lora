#!/bin/bash -l
#SBATCH --job-name=train_lora
#SBATCH --ntasks=1
#SBATCH --gres=gpu:a100:8
#SBATCH --partition=a100
#SBATCH --output=R-%x.%j.out
#SBATCH --error=R-%x.%j.err
#SBATCH --mail-type=end,fail
#SBATCH --time=24:00:00
#SBATCH --export=NONE
unset SLURM_EXPORT_ENV

source ~/.bashrc

# Set proxy to access internet from the node
export http_proxy=http://proxy:80
export https_proxy=http://proxy:80

module purge
module load python/3.9-anaconda
module load cudnn/8.2.4.15-11.4
module load cuda/11.4.2

# Conda
source activate chest

# create a temporary job dir on $WORK
mkdir ${WORK}/$SLURM_JOB_ID

# copy input file from location where job was submitted, and run
cp -r ${SLURM_SUBMIT_DIR}/. .

# Run training script (with data copied to node)
srun launch_training_hpc
